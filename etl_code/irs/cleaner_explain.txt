This spark job should be ran by copy and pasting the commands from Cleaner.scalaspark into a spark shell.  The directories will have to be updated.  The input of the job is IRS  csv files and the output of the job is a text file saved to HDFS and a parquet file saved to HDFS to be used in modeling. The cleaning which occurs is combining multiple csv files into one rdd and taking the correct columns.
