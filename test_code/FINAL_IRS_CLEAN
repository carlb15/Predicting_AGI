val irsRaw11 = spark.read.option("header", "true").csv("SHARE/IRS/RAW/11incyallagi.csv")
val irsRaw12 = spark.read.option("header", "true").csv("SHARE/IRS/RAW/12cyallagi.csv")
val irsRaw11 = spark.read.option("header", "true").csv("SHARE/IRS/RAW/11incyallagi.csv").select("STATEFIPS", "STATE", "COUNTYFIPS", "COUNTYNAME", "agi_stub", "N1", "A00100")
val irsRaw12 = spark.read.option("header", "true").csv("SHARE/IRS/RAW/12cyallagi.csv").select("STATEFIPS", "STATE", "COUNTYFIPS", "COUNTYNAME", "agi_stub", "N1", "A00100")
val irsRaw13 = spark.read.option("header", "true").csv("SHARE/IRS/RAW/13incyallagi.csv").select("STATEFIPS", "STATE", "COUNTYFIPS", "COUNTYNAME", "agi_stub", "N1", "A00100")
val irsRaw14 = spark.read.option("header", "true").csv("SHARE/IRS/RAW/14incyallagi.csv").select("STATEFIPS", "STATE", "COUNTYFIPS", "COUNTYNAME", "agi_stub", "N1", "A00100")
val irsRaw15 = spark.read.option("header", "true").csv("SHARE/IRS/RAW/15incyallagi.csv").select("STATEFIPS", "STATE", "COUNTYFIPS", "COUNTYNAME", "agi_stub", "N1", "A00100")
val irsRaw16 = spark.read.option("header", "true").csv("SHARE/IRS/RAW/16incyallagi.csv").select("STATEFIPS", "STATE", "COUNTYFIPS", "COUNTYNAME", "agi_stub", "N1", "A00100")
val irsRaw17 = spark.read.option("header", "true").csv("SHARE/IRS/RAW/17incyallagi.csv").select("STATEFIPS", "STATE", "COUNTYFIPS", "COUNTYNAME", "agi_stub", "N1", "A00100")
val toAdd = Array(irsRaw12, irsRaw13, irsRaw14, irsRaw15, irsRaw16, irsRaw17)
var irsRaw = irsRaw11
for (irs <- toAdd) { irsRaw = irsRaw.union(irs) }
import org.apache.spark.sql.functions.udf
import org.apache.spark.sql.functions.input_file_name
def extractYear = udf((filename: String) => "20" + filename.split("/").last.substring(0,2))
val irsLess = irsRaw.select($"STATEFIPS".cast("Int"), $"STATE", $"COUNTYFIPS".cast("Int"), $"COUNTYNAME", $"agi_stub".cast("Int"), $"N1".cast("Int"), $"A00100".cast("Int")).na.drop.withColumn("year", extractYear(input_file_name))
irsLess.show
val irsNoTotals = irsLess.filter($"COUNTYFIPS" =!= 0)
def makeCountyID = udf((statefips:Int, countyfips:Int) => f"$statefips%02d$countyfips%03d")
val irsWithCombinedID = irsNoTotals.withColumn("county_id", makeCountyID($"STATEFIPS", $"COUNTYFIPS"))
irsWithCombinedID.show
:load IrsProfile.scala
import spark.implicits._
implicit val IrsProfileEncoder = org.apache.spark.sql.Encoders.kryo[IrsProfile]
val irsFinal = irsWithCombinedID.map(r => IrsProfileFactory(r)).groupByKey(ip => (ip.fipstate, ip.fipscty, ip.year)).reduceGroups(_.merge(_)).map(r => (r._2.fipstate, r._2.state, r._2.fipscty, r._2.county, r._2.county_id, r._2.n1, r._2.agi, r._2.year))
irsFinal.show
val irsFinal = irsWithCombinedID.map(r => IrsProfileFactory(r)).groupByKey(ip => (ip.fipstate, ip.fipscty, ip.year)).reduceGroups(_.merge(_)).map(r => (r._2.fipstate, r._2.state, r._2.fipscty, r._2.county, r._2.county_id, r._2.n1, r._2.agi, r._2.year)).withColumnRenamed("_1", "fipstate").withColumnRenamed("_2", "state").withColumnRenamed("_3", "fipscty").withColumnRenamed("_4", "county").withColumnRenamed("_5", "state_county").withColumnRenamed("_6", "n1").withColumnRenamed("_7", "mars1").withColumnRenamed("_8", "mars2").withColumnRenamed("_9", "mars4").withColumnRenamed("_10", "agi").withColumnRenamed("_11", "year")
irsFinal.show
val irsFinal = irsWithCombinedID.map(r => IrsProfileFactory(r)).groupByKey(ip => (ip.fipstate, ip.fipscty, ip.year)).reduceGroups(_.merge(_)).map(r => (r._2.fipstate, r._2.state, r._2.fipscty, r._2.county, r._2.county_id, r._2.n1, r._2.agi, r._2.year)).withColumnRenamed("_1", "fipstate").withColumnRenamed("_2", "state").withColumnRenamed("_3", "fipscty").withColumnRenamed("_4", "county").withColumnRenamed("_5", "state_county").withColumnRenamed("_6", "n1").withColumnRenamed("_7", "agi").withColumnRenamed("8", "year")
irsFinal.show
val irsFinal = irsWithCombinedID.map(r => IrsProfileFactory(r)).groupByKey(ip => (ip.fipstate, ip.fipscty, ip.year)).reduceGroups(_.merge(_)).map(r => (r._2.fipstate, r._2.state, r._2.fipscty, r._2.county, r._2.county_id, r._2.n1, r._2.agi, r._2.year)).withColumnRenamed("_1", "fipstate").withColumnRenamed("_2", "state").withColumnRenamed("_3", "fipscty").withColumnRenamed("_4", "county").withColumnRenamed("_5", "state_county").withColumnRenamed("_6", "n1").withColumnRenamed("_7", "agi").withColumnRenamed("_8", "year")
irsFinal.show
irsFinal.filter($"fipstate" === 1 && $"fipscty" === 1 && ($"year" === 2011 || $"year" === 2017)).show
irsFinal.write.parquet("SHARE/DF/irs_new.parquet")
irsFinal.count
val cbp = spark.read.parquet("SHARE/DF/cbp.parquet")
cbp.count
cbp.join(irsFinal, cbp.col("fips_combined") === irsFinal.col("state_county"))
cbp.join(irsFinal, cbp.col("fips_combined") === irsFinal.col("state_county")).count
cbp.count
irsFinal.count
val t = irsFinal.withColumnRenamed("state_county", "fips_combined")
cbp.join(t, "fips_combined")
cbp.join(t, "fips_combined").count
cbp.printSchema
t.printSchema
res18.show
153789 / 6
cbp.join(irsFinal, cbp.col("fips_combined") === irsFinal.col("state_county") && cbp.col("year") === irsFinal.col("year"))
cbp.join(irsFinal, cbp.col("fips_combined") === irsFinal.col("state_county") && cbp.col("year") === irsFinal.col("year")).count
res27 / 7
res26.show
val t = irsFinal.withColumnRenamed("state_county", "fips_combined")
cbp.join(t, Seq("fipstate", "fipscty", "year", "fips_combined"), "inner")
res31.count
res31.show
